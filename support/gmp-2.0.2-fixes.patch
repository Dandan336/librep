--- gmp-2.0.2-vanilla/insert-double.c	Tue Jun  4 09:27:12 1996
+++ gmp-2.0.2/insert-double.c	Sat Nov 14 08:07:14 1998
@@ -43,7 +43,25 @@
   {
     union ieee_double_extract x;
     x.d = d;
-    x.s.exp += exp;
+    exp += x.s.exp; 
+    x.s.exp = exp; 
+    if (exp >= 2047) 
+      { 
+	/* Return +-infinity */ 
+	x.s.exp = 2047; 
+	x.s.manl = x.s.manh = 0; 
+      } 
+    else if (exp < 1) 
+      { 
+	x.s.exp = 1;		/* smallest exponent (biased) */ 
+	/* Divide result by 2 until we have scaled it to the right IEEE 
+	   denormalized number, but stop if it becomes zero.  */ 
+	while (exp < 1 && x.d != 0) 
+	  { 
+	    x.d *= 0.5; 
+	    exp++; 
+	  } 
+      }
     return x.d;
   }
 #else
@@ -57,12 +75,18 @@
 	exp = -exp;
       }
     r = d;
-    while (exp != 0)
+    if (exp != 0)
       {
 	if ((exp & 1) != 0)
 	  r *= factor;
-	factor *= factor;
 	exp >>= 1;
+	while (exp != 0) 
+	  { 
+	    factor *= factor; 
+	    if ((exp & 1) != 0) 
+	      r *= factor; 
+	    exp >>= 1; 
+	  }
       }
     return r;
   }
--- gmp-2.0.2-vanilla/longlong.h	Fri May 24 08:34:24 1996
+++ gmp-2.0.2/longlong.h	Thu Jul  1 11:01:10 1999
@@ -1145,6 +1145,72 @@
 #endif /* udiv_qrnnd */
 #endif /* __sparc__ */
 
+#if (defined (__sparc_v9__) || (defined (__sparc__) && defined (__arch64__)) \
+    || defined (__sparcv9)) && W_TYPE_SIZE == 64
+#define add_ssaaaa(sh, sl, ah, al, bh, bl)				\
+  __asm__ ("addcc %4,%5,%1
+  	    add %2,%3,%0
+  	    bcs,a,pn %%xcc, 1f
+  	    add %0, 1, %0
+  	    1:"								\
+	   : "=r" ((UDItype)(sh)),				      	\
+	     "=&r" ((UDItype)(sl))				      	\
+	   : "r" ((UDItype)(ah)),				     	\
+	     "r" ((UDItype)(bh)),				      	\
+	     "r" ((UDItype)(al)),				     	\
+	     "r" ((UDItype)(bl))				       	\
+	   : "cc")
+
+#define sub_ddmmss(sh, sl, ah, al, bh, bl) 				\
+  __asm__ ("subcc %4,%5,%1
+  	    sub %2,%3,%0
+  	    bcs,a,pn %%xcc, 1f
+  	    sub %0, 1, %0
+  	    1:"								\
+	   : "=r" ((UDItype)(sh)),				      	\
+	     "=&r" ((UDItype)(sl))				      	\
+	   : "r" ((UDItype)(ah)),				     	\
+	     "r" ((UDItype)(bh)),				      	\
+	     "r" ((UDItype)(al)),				     	\
+	     "r" ((UDItype)(bl))				       	\
+	   : "cc")
+
+#define umul_ppmm(wh, wl, u, v)						\
+  do {									\
+	  UDItype tmp1, tmp2, tmp3, tmp4;				\
+	  __asm__ __volatile__ (					\
+		   "srl %7,0,%3
+		    mulx %3,%6,%1
+		    srlx %6,32,%2
+		    mulx %2,%3,%4
+		    sllx %4,32,%5
+		    srl %6,0,%3
+		    sub %1,%5,%5
+		    srlx %5,32,%5
+		    addcc %4,%5,%4
+		    srlx %7,32,%5
+		    mulx %3,%5,%3
+		    mulx %2,%5,%5
+		    sethi %%hi(0x80000000),%2
+		    addcc %4,%3,%4
+		    srlx %4,32,%4
+		    add %2,%2,%2
+		    movcc %%xcc,%%g0,%2
+		    addcc %5,%4,%5
+		    sllx %3,32,%3
+		    add %1,%3,%1
+		    add %5,%2,%0"					\
+	   : "=r" ((UDItype)(wh)),					\
+	     "=&r" ((UDItype)(wl)),					\
+	     "=&r" (tmp1), "=&r" (tmp2), "=&r" (tmp3), "=&r" (tmp4)	\
+	   : "r" ((UDItype)(u)),					\
+	     "r" ((UDItype)(v))						\
+	   : "cc");							\
+  } while (0)
+#define UMUL_TIME 96
+#define UDIV_TIME 230
+#endif /* __sparc_v9__ */
+
 #if defined (__vax__) && W_TYPE_SIZE == 32
 #define add_ssaaaa(sh, sl, ah, al, bh, bl) \
   __asm__ ("addl2 %5,%1
--- gmp-2.0.2-vanilla/mpf/get_str.c	Wed May  8 09:11:08 1996
+++ gmp-2.0.2/mpf/get_str.c	Wed Nov 11 18:33:45 1998
@@ -28,32 +28,21 @@
 #include "longlong.h"
 
 /*
-   New algorithm for converting fractions (951019):
-   0. Call the fraction to convert F.
-   1. Compute [exp * log(2^BITS_PER_MP_LIMB)/log(B)], i.e.,
-      [exp * BITS_PER_MP_LIMB * __mp_bases[B].chars_per_bit_exactly].  Exp is
-      the number of limbs between the limb point and the most significant
-      non-zero limb.  Call this result n.
-   2. Compute B^n.
-   3. F*B^n will now be just below 1, which can be converted easily.  (Just
-      multiply by B repeatedly, and see the digits fall out as integers.)
-   We should interrupt the conversion process of F*B^n as soon as the number
-   of digits requested have been generated.
-
-   New algorithm for converting integers (951019):
-   0. Call the integer to convert I.
-   1. Compute [exp * log(2^BITS_PER_MP_LIMB)/log(B)], i.e.,
-      [exp BITS_PER_MP_LIMB * __mp_bases[B].chars_per_bit_exactly].  Exp is
-      the number of limbs between the limb point and the least significant
-      non-zero limb.  Call this result n.
-   2. Compute B^n.
-   3. I/B^n can be converted easily.  (Just divide by B repeatedly.  In GMP,
-      this is best done by calling mpn_get_str.)
-   Note that converting I/B^n could yield more digits than requested.  For
-   efficiency, the variable n above should be set larger in such cases, to
-   kill all undesired digits in the division in step 3.
+  The conversion routine works like this:
+
+  1. If U >= 1, compute U' = U / base**n, where n is chosen such that U' is
+     the largest number smaller than 1.
+  2. Else, if U < 1, compute U' = U * base**n, where n is chosen such that U'
+     is the largest number smaller than 1.
+  3. Convert U' (by repeatedly multiplying it by base).  This process can
+     easily be interrupted when the needed number of digits are generated.
 */
 
+#define assert(true) do { if (!(true)) abort (); } while (0)
+
+#define swapptr(xp,yp) \
+do { mp_ptr _swapptr_tmp = (xp); (xp) = (yp); (yp) = _swapptr_tmp; } while (0)
+
 char *
 #if __STDC__
 mpf_get_str (char *digit_ptr, mp_exp_t *exp, int base, size_t n_digits, mpf_srcptr u)
@@ -66,24 +55,26 @@
      mpf_srcptr u;
 #endif
 {
+  mp_ptr up;
   mp_size_t usize;
   mp_exp_t uexp;
+  mp_size_t prec;
   unsigned char *str;
-  size_t str_size;
   char *num_to_text;
-  long i;			/* should be size_t */
   mp_ptr rp;
+  mp_size_t rsize;
   mp_limb_t big_base;
   size_t digits_computed_so_far;
   int dig_per_u;
-  mp_srcptr up;
   unsigned char *tstr;
   mp_exp_t exp_in_base;
+  int cnt;
   TMP_DECL (marker);
 
   TMP_MARK (marker);
   usize = u->_mp_size;
   uexp = u->_mp_exp;
+  prec = u->_mp_prec;
 
   if (base >= 0)
     {
@@ -101,8 +92,8 @@
      Also, if 0 digits were requested, give *exactly* as many digits
      as can be accurately represented.  */
   {
-    size_t max_digits = (((u->_mp_prec - 1) * BITS_PER_MP_LIMB)
-			 * __mp_bases[base].chars_per_bit_exactly);
+    size_t max_digits = 2 + (size_t) (((prec - 1) * BITS_PER_MP_LIMB)
+				      * __mp_bases[base].chars_per_bit_exactly);
     if (n_digits == 0 || n_digits > max_digits)
       n_digits = max_digits;
   }
@@ -123,10 +114,6 @@
 
   str = (unsigned char *) digit_ptr;
 
-  /* Allocate temporary digit space.  We can't put digits directly in the user
-     area, since we almost always generate more digits than requested.  */
-  tstr = (unsigned char *) TMP_ALLOC (n_digits + 3 * BITS_PER_MP_LIMB);
-
   if (usize < 0)
     {
       *digit_ptr = '-';
@@ -134,367 +121,288 @@
       usize = -usize;
     }
 
-  digits_computed_so_far = 0;
+  up = PTR (u);
 
-  if (uexp > usize)
+  if (uexp > 0)
     {
-      /* The number has just an integral part.  */
-      mp_size_t rsize;
-      mp_size_t exp_in_limbs;
-      mp_size_t msize;
-      mp_ptr tp, xp, mp;
-      int cnt;
-      mp_limb_t cy;
-      mp_size_t start_str;
-      mp_size_t n_limbs;
+      /* U >= 1.  Compute U' = U / base**n, where n is chosen such that U' < 1.  */
+      mp_size_t ralloc;
+      mp_ptr tp;
+      int i;
 
-      n_limbs = 2 + ((mp_size_t) (n_digits / __mp_bases[base].chars_per_bit_exactly)
-		     / BITS_PER_MP_LIMB);
+      /* Limit the number of digits to develop for small integers.  */
+#if 0
+      if (exp_in_base < n_digits)
+	n_digits = exp_in_base;
+#endif
 
-      /* Compute n such that [u/B^n] contains (somewhat) more than n_digits
-	 digits.  (We compute less than that only if that is an exact number,
-	 i.e., exp is small enough.)  */
+      count_leading_zeros (cnt, up[usize - 1]);
+      exp_in_base = ((uexp * BITS_PER_MP_LIMB - cnt)
+		     * __mp_bases[base].chars_per_bit_exactly);
+      exp_in_base += 1;
+
+      ralloc = uexp + 2;
+      rp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
+      tp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
+
+      rp[0] = base;
+      rsize = 1;
+      count_leading_zeros (cnt, exp_in_base);
+      for (i = BITS_PER_MP_LIMB - cnt - 2; i >= 0; i--)
+	{
+	  mpn_mul_n (tp, rp, rp, rsize);
+	  rsize = 2 * rsize;
+	  rsize -= tp[rsize - 1] == 0;
+	  swapptr (rp, tp);
 
-      exp_in_limbs = uexp;
+	  if (((exp_in_base >> i) & 1) != 0)
+	    {
+	      mp_limb_t cy;
+	      cy = mpn_mul_1 (rp, rp, rsize, (mp_limb_t) base);
+	      rp[rsize] = cy;
+	      rsize += cy != 0;
+	    }
+	}
 
-      if (n_limbs >= exp_in_limbs)
+      count_leading_zeros (cnt, rp[rsize - 1]);
+      if (cnt != 0)
 	{
-	  /* The number is so small that we convert the entire number.  */
-	  exp_in_base = 0;
-	  rp = (mp_ptr) TMP_ALLOC (exp_in_limbs * BYTES_PER_MP_LIMB);
-	  MPN_ZERO (rp, exp_in_limbs - usize);
-	  MPN_COPY (rp + (exp_in_limbs - usize), u->_mp_d, usize);
-	  rsize = exp_in_limbs;
+	  mpn_lshift (rp, rp, rsize, cnt);
+
+	  if (usize < rsize)
+	    {
+	      /* Pad out U to the size of R while shifting it.
+		 (Reuse temporary space at tp.)  */
+	      mp_limb_t cy;
+
+	      MPN_ZERO (tp, rsize - usize);
+	      cy = mpn_lshift (tp + rsize - usize, up, usize, cnt);
+	      up = tp;
+	      usize = rsize;
+	      if (cy)
+		up[usize++] = cy;
+	      assert (usize <= ralloc);	/* sufficient space? */
+	    }
+	  else
+	    {
+	      /* Copy U to temporary space.  */
+	      /* FIXME: Allocate more space for tp above, and reuse it here.  */
+	      mp_limb_t cy;
+	      mp_ptr tup = (mp_ptr) TMP_ALLOC ((usize + 1) * BYTES_PER_MP_LIMB);
+
+	      cy = mpn_lshift (tup, up, usize, cnt);
+	      up = tup;
+	      if (cy)
+		up[usize++] = cy;
+	    }
 	}
       else
 	{
-	  exp_in_limbs -= n_limbs;
-	  exp_in_base = (((exp_in_limbs * BITS_PER_MP_LIMB - 1))
-			 * __mp_bases[base].chars_per_bit_exactly);
-
-	  rsize = exp_in_limbs + 1;
-	  rp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
-	  tp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
+	  if (usize < rsize)
+	    {
+	      /* Pad out U to the size of R.  (Reuse temporary space at tp.)  */
+	      MPN_ZERO (tp, rsize - usize);
+	      MPN_COPY (tp + rsize - usize, up, usize);
+	      up = tp;
+	      usize = rsize;
+	    }
+	  else
+	    {
+	      /* Copy U to temporary space.  */
+	      mp_ptr tmp = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
+	      MPN_COPY (tmp, up, usize);
+	      up = tmp;
+	    }
+	}
+
+      {
+	mp_ptr qp;
+	qp = (mp_ptr) TMP_ALLOC ((prec + 1) * BYTES_PER_MP_LIMB);
+	mpn_divrem (qp, prec + 1 - (usize - rsize), up, usize, rp, rsize);
+	rsize = prec + 1;
+	rp = qp;
+      }
+    }
+  else
+    {
+      /* U < 1.  Compute U' = U * base**n, where n is chosen such that U' is
+	 the greatest number that still satisfies U' < 1.  */
+      mp_size_t ralloc;
+      mp_ptr tp;
+      int i;
+
+      uexp = -uexp;
+      count_leading_zeros (cnt, up[usize - 1]);
+      exp_in_base = ((uexp * BITS_PER_MP_LIMB + cnt - 1)
+		     * __mp_bases[base].chars_per_bit_exactly);
+      if (exp_in_base < 0)
+	exp_in_base = 0;
+
+      if (exp_in_base != 0)
+	{
+	  ralloc = uexp + 2;
+	  rp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
+	  tp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
 
 	  rp[0] = base;
 	  rsize = 1;
-
 	  count_leading_zeros (cnt, exp_in_base);
 	  for (i = BITS_PER_MP_LIMB - cnt - 2; i >= 0; i--)
 	    {
 	      mpn_mul_n (tp, rp, rp, rsize);
 	      rsize = 2 * rsize;
 	      rsize -= tp[rsize - 1] == 0;
-	      xp = tp; tp = rp; rp = xp;
+	      swapptr (rp, tp);
 
 	      if (((exp_in_base >> i) & 1) != 0)
 		{
+		  mp_limb_t cy;
 		  cy = mpn_mul_1 (rp, rp, rsize, (mp_limb_t) base);
 		  rp[rsize] = cy;
 		  rsize += cy != 0;
 		}
 	    }
 
-	  mp = u->_mp_d;
-	  msize = usize;
-
 	  {
-	    mp_ptr qp;
-	    mp_limb_t qflag;
-	    mp_size_t xtra;
-	    if (msize < rsize)
-	      {
-		mp_ptr tmp = (mp_ptr) TMP_ALLOC ((rsize+1)* BYTES_PER_MP_LIMB);
-		MPN_ZERO (tmp, rsize - msize);
-		MPN_COPY (tmp + rsize - msize, mp, msize);
-		mp = tmp;
-		msize = rsize;
-	      }
+	    mp_limb_t cy;
+	    tp = (mp_ptr) TMP_ALLOC ((rsize + usize) * BYTES_PER_MP_LIMB);
+	    if (rsize > usize)
+	      cy = mpn_mul (tp, rp, rsize, up, usize);
 	    else
-	      {
-		mp_ptr tmp = (mp_ptr) TMP_ALLOC ((msize+1)* BYTES_PER_MP_LIMB);
-		MPN_COPY (tmp, mp, msize);
-		mp = tmp;
-	      }
-	    count_leading_zeros (cnt, rp[rsize - 1]);
-	    cy = 0;
-	    if (cnt != 0)
-	      {
-		mpn_lshift (rp, rp, rsize, cnt);
-		cy = mpn_lshift (mp, mp, msize, cnt);
-		if (cy)
-		  mp[msize++] = cy;
-	      }
-
-	    {
-	      mp_size_t qsize = n_limbs + (cy != 0);
-	      qp = (mp_ptr) TMP_ALLOC ((qsize + 1) * BYTES_PER_MP_LIMB);
-	      xtra = qsize - (msize - rsize);
-	      qflag = mpn_divrem (qp, xtra, mp, msize, rp, rsize);
-	      qp[qsize] = qflag;
-	      rsize = qsize + qflag;
-	      rp = qp;
-	    }
+	      cy = mpn_mul (tp, up, usize, rp, rsize);
+	    rsize += usize;
+	    rsize -= cy == 0;
+	    rp = tp;
 	  }
+	  exp_in_base = -exp_in_base;
 	}
-
-      str_size = mpn_get_str (tstr, base, rp, rsize);
-
-      if (str_size > n_digits + 3 * BITS_PER_MP_LIMB)
-	abort ();
-
-      start_str = 0;
-      while (tstr[start_str] == 0)
-	start_str++;
-
-      for (i = start_str; i < str_size; i++)
+      else
 	{
-	  tstr[digits_computed_so_far++] = tstr[i];
-	  if (digits_computed_so_far > n_digits)
-	    break;
+	  rp = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
+	  MPN_COPY (rp, up, usize);
+	  rsize = usize;
 	}
-      exp_in_base = exp_in_base + str_size - start_str;
-      goto finish_up;
     }
 
-  exp_in_base = 0;
+  big_base = __mp_bases[base].big_base;
+  dig_per_u = __mp_bases[base].chars_per_limb;
 
-  if (uexp > 0)
+  /* Hack for correctly (although not optimally) converting to bases that are
+     powers of 2.  If we deem it important, we could handle powers of 2 by
+     shifting and masking (just like mpn_get_str).  */
+  if (big_base < 10)		/* logarithm of base when power of two */
     {
-      /* The number has an integral part, convert that first.
-	 If there is a fractional part too, it will be handled later.  */
-      mp_size_t start_str;
-
-      rp = (mp_ptr) TMP_ALLOC (uexp * BYTES_PER_MP_LIMB);
-      up = u->_mp_d + usize - uexp;
-      MPN_COPY (rp, up, uexp);
-
-      str_size = mpn_get_str (tstr, base, rp, uexp);
+      int logbase = big_base;
+      if (dig_per_u * logbase == BITS_PER_MP_LIMB)
+	dig_per_u--;
+      big_base = (mp_limb_t) 1 << (dig_per_u * logbase);
+      /* fall out to general code... */
+    }
 
-      start_str = 0;
-      while (tstr[start_str] == 0)
-	start_str++;
+  /* Allocate temporary digit space.  We can't put digits directly in the user
+     area, since we generate more digits than requested.  (We allocate
+     BITS_PER_MP_LIMB + 1 extra bytes because of the digit block nature of the
+     conversion.)  */
+  tstr = (unsigned char *) TMP_ALLOC (n_digits + BITS_PER_MP_LIMB + 1);
 
-      for (i = start_str; i < str_size; i++)
+  for (digits_computed_so_far = 0; digits_computed_so_far <= n_digits;
+       digits_computed_so_far += dig_per_u)
+    {
+      mp_limb_t cy;
+      /* For speed: skip trailing zeroes.  */
+      if (rp[0] == 0)
 	{
-	  tstr[digits_computed_so_far++] = tstr[i];
-	  if (digits_computed_so_far > n_digits)
+	  rp++;
+	  rsize--;
+	  if (rsize == 0)
 	    {
-	      exp_in_base = str_size - start_str;
-	      goto finish_up;
+	      n_digits = digits_computed_so_far;
+	      break;
 	    }
 	}
 
-      exp_in_base = str_size - start_str;
-      /* Modify somewhat and fall out to convert fraction... */
-      usize -= uexp;
-      uexp = 0;
-    }
-
-  if (usize <= 0)
-    goto finish_up;
-
-  /* Convert the fraction.  */
-  {
-    mp_size_t rsize, msize;
-    mp_ptr rp, tp, xp, mp;
-    int cnt;
-    mp_limb_t cy;
-    mp_exp_t nexp;
-
-    big_base = __mp_bases[base].big_base;
-    dig_per_u = __mp_bases[base].chars_per_limb;
-
-    /* Hack for correctly (although not efficiently) converting to bases that
-       are powers of 2.  If we deem it important, we could handle powers of 2
-       by shifting and masking (just like mpn_get_str).  */
-    if (big_base < 10)		/* logarithm of base when power of two */
-      {
-	int logbase = big_base;
-	if (dig_per_u * logbase == BITS_PER_MP_LIMB)
-	  dig_per_u--;
-	big_base = (mp_limb_t) 1 << (dig_per_u * logbase);
-	/* fall out to general code... */
-      }
-
-#if 0
-    if (0 && uexp == 0)
-      {
-	rp = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
-	up = u->_mp_d;
-	MPN_COPY (rp, up, usize);
-	rsize = usize;
-	nexp = 0;
-      }
-    else
-      {}
-#endif
-    uexp = -uexp;
-    if (u->_mp_d[usize - 1] == 0)
-      cnt = 0;
-    else
-      count_leading_zeros (cnt, u->_mp_d[usize - 1]);
+      cy = mpn_mul_1 (rp, rp, rsize, big_base);
 
-    nexp = ((uexp * BITS_PER_MP_LIMB) + cnt)
-      * __mp_bases[base].chars_per_bit_exactly;
+      assert (! (digits_computed_so_far == 0 && cy == 0));
 
-    if (nexp == 0)
+      /* Convert N1 from BIG_BASE to a string of digits in BASE
+	 using single precision operations.  */
       {
-	rp = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
-	up = u->_mp_d;
-	MPN_COPY (rp, up, usize);
-	rsize = usize;
-      }
-    else
-      {
-	rsize = uexp + 2;
-	rp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
-	tp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
-
-	rp[0] = base;
-	rsize = 1;
-
-	count_leading_zeros (cnt, nexp);
-	for (i = BITS_PER_MP_LIMB - cnt - 2; i >= 0; i--)
-	  {
-	    mpn_mul_n (tp, rp, rp, rsize);
-	    rsize = 2 * rsize;
-	    rsize -= tp[rsize - 1] == 0;
-	    xp = tp; tp = rp; rp = xp;
-
-	    if (((nexp >> i) & 1) != 0)
-	      {
-		cy = mpn_mul_1 (rp, rp, rsize, (mp_limb_t) base);
-		rp[rsize] = cy;
-		rsize += cy != 0;
-	      }
-	  }
-
-	/* Did our multiplier (base^nexp) cancel with uexp?  */
-#if 0
-	if (uexp != rsize)
-	  {
-	    do
-	      {
-		cy = mpn_mul_1 (rp, rp, rsize, big_base);
-		nexp += dig_per_u;
-	      }
-	    while (cy == 0);
-	    rp[rsize++] = cy;
-	  }
-#endif
-	mp = u->_mp_d;
-	msize = usize;
-
-	tp = (mp_ptr) TMP_ALLOC ((rsize + msize) * BYTES_PER_MP_LIMB);
-	if (rsize > msize)
-	  cy = mpn_mul (tp, rp, rsize, mp, msize);
-	else
-	  cy = mpn_mul (tp, mp, msize, rp, rsize);
-	rsize += msize;
-	rsize -= cy == 0;
-	rp = tp;
-
-	/* If we already output digits (for an integral part) pad
-	   leading zeros.  */
-	if (digits_computed_so_far != 0)
-	  for (i = 0; i < nexp; i++)
-	    tstr[digits_computed_so_far++] = 0;
-      }
-
-    while (digits_computed_so_far <= n_digits)
-      {
-	/* For speed: skip trailing zeroes.  */
-	if (rp[0] == 0)
-	  {
-	    rp++;
-	    rsize--;
-	    if (rsize == 0)
-	      {
-		n_digits = digits_computed_so_far;
-		break;
-	      }
-	  }
-
-	cy = mpn_mul_1 (rp, rp, rsize, big_base);
-	if (digits_computed_so_far == 0 && cy == 0)
+	int i;
+	unsigned char *s = tstr + digits_computed_so_far + dig_per_u;
+	for (i = dig_per_u - 1; i >= 0; i--)
 	  {
-	    abort ();
-	    nexp += dig_per_u;
-	    continue;
+	    *--s = cy % base;
+	    cy /= base;
 	  }
-	/* Convert N1 from BIG_BASE to a string of digits in BASE
-	   using single precision operations.  */
-	{
-	  unsigned char *s = tstr + digits_computed_so_far + dig_per_u;
-	  for (i = dig_per_u - 1; i >= 0; i--)
-	    {
-	      *--s = cy % base;
-	      cy /= base;
-	    }
-	}
-	digits_computed_so_far += dig_per_u;
       }
-    if (exp_in_base == 0)
-      exp_in_base = -nexp;
-  }
-
- finish_up:
+    }
 
-  /* We can have at most one leading 0.  Remove it.  */
+  /* We can have at most two leading 0.  Remove them.  */
   if (tstr[0] == 0)
     {
       tstr++;
       digits_computed_so_far--;
       exp_in_base--;
-    }
 
-  /* We should normally have computed too many digits.  Round the result
-     at the point indicated by n_digits.  */
-  if (digits_computed_so_far > n_digits)
-    {
-      /* Round the result.  */
-      if (tstr[n_digits] * 2 >= base)
+      if (tstr[0] == 0)
 	{
-	  digits_computed_so_far = n_digits;
-	  for (i = n_digits - 1; i >= 0; i--)
-	    {
-	      unsigned int x;
-	      x = ++(tstr[i]);
-	      if (x < base)
-		goto rounded_ok;
-	      digits_computed_so_far--;
-	    }
-	  tstr[0] = 1;
-	  digits_computed_so_far = 1;
-	  exp_in_base++;
-	rounded_ok:;
+	  tstr++;
+	  digits_computed_so_far--;
+	  exp_in_base--;
+
+	  if (tstr[0] == 0)
+	    abort ();
 	}
     }
 
-  /* We might have fewer digits than requested as a result of rounding above,
-     (i.e. 0.999999 => 1.0) or because we have a number that simply doesn't
-     need many digits in this base (i.e., 0.125 in base 10).  */
-  if (n_digits > digits_computed_so_far)
-    n_digits = digits_computed_so_far;
-
-  /* Remove trailing 0.  There can be many zeros. */
-  while (n_digits != 0 && tstr[n_digits - 1] == 0)
-    n_digits--;
-
-  /* Translate to ascii and null-terminate.  */
-  for (i = 0; i < n_digits; i++)
-    *str++ = num_to_text[tstr[i]];
+  {
+    size_t i;
+
+    /* We should normally have computed too many digits.  Round the result
+       at the point indicated by n_digits.  */
+    if (digits_computed_so_far > n_digits)
+      {
+	/* Round the result.  */
+	if (tstr[n_digits] * 2 >= base)
+	  {
+	    digits_computed_so_far = n_digits;
+	    for (i = n_digits - 1;; i--)
+	      {
+		unsigned int x;
+		x = ++(tstr[i]);
+		if (x != base)
+		  break;
+		digits_computed_so_far--;
+		if (i == 0)
+		  {
+		    /* We had something like `9999999...9d', where 2*d >= base.
+		       This rounds up to `1', increasing the exponent.  */
+		    tstr[0] = 1;
+		    digits_computed_so_far = 1;
+		    exp_in_base++;
+		    break;
+		  }
+	      }
+	  }
+      }
+
+    /* We might have fewer digits than requested as a result of rounding above,
+       (i.e. 0.999999 => 1.0) or because we have a number that simply doesn't
+       need many digits in this base (i.e., 0.125 in base 10).  */
+    if (n_digits > digits_computed_so_far)
+      n_digits = digits_computed_so_far;
+
+    /* Remove trailing 0.  There can be many zeros.  */
+    while (n_digits != 0 && tstr[n_digits - 1] == 0)
+      n_digits--;
+
+    /* Translate to ascii and null-terminate.  */
+    for (i = 0; i < n_digits; i++)
+      *str++ = num_to_text[tstr[i]];
+  }
   *str = 0;
   *exp = exp_in_base;
   TMP_FREE (marker);
   return digit_ptr;
 }
-
-#if COPY_THIS_TO_OTHER_PLACES
-      /* Use this expression in lots of places in the library instead of the
-	 count_leading_zeros+expression that is used currently.  This expression
-	 is much more accurate and will save odles of memory.  */
-      rsize = ((mp_size_t) (exp_in_base / __mp_bases[base].chars_per_bit_exactly)
-	       + BITS_PER_MP_LIMB) / BITS_PER_MP_LIMB;
-#endif
--- gmp-2.0.2-vanilla/mpf/set_q.c	Fri May 24 08:34:29 1996
+++ gmp-2.0.2/mpf/set_q.c	Tue Jul 13 15:11:28 1999
@@ -164,7 +164,7 @@
     }
 
   EXP (r) = exp;
-  SIZ (r) = qsize;
+  SIZ (r) = sign_quotient >= 0 ? qsize : -qsize;
 
   TMP_FREE (marker);
 }
--- gmp-2.0.2-vanilla/mpf/set_str.c	Wed May  8 09:11:09 1996
+++ gmp-2.0.2/mpf/set_str.c	Wed Nov 11 18:33:45 1998
@@ -27,6 +27,11 @@
 #include "gmp-impl.h"
 #include "longlong.h"
 
+#define assert(true) do { if (!(true)) abort (); } while (0)
+
+#define swapptr(xp,yp) \
+do { mp_ptr _swapptr_tmp = (xp); (xp) = (yp); (yp) = _swapptr_tmp; } while (0)
+
 long int strtol _PROTO ((const char *, char **ptr, int));
 
 static int
@@ -159,14 +164,13 @@
   xsize = str_size / __mp_bases[base].chars_per_limb + 2;
   {
     long exp_in_base;
-    mp_size_t rsize, msize;
+    mp_size_t ralloc, rsize, msize;
     int cnt, i;
-    mp_ptr mp, xp, tp, rp;
-    mp_limb_t cy;
+    mp_ptr mp, tp, rp;
     mp_exp_t exp_in_limbs;
-    mp_size_t prec = x->_mp_prec;
+    mp_size_t prec = x->_mp_prec + 1;
     int divflag;
-    mp_size_t xxx = 0;
+    mp_size_t madj, radj;
 
     mp = (mp_ptr) TMP_ALLOC (xsize * BYTES_PER_MP_LIMB);
     msize = mpn_set_str (mp, (unsigned char *) begs, str_size, base);
@@ -179,6 +183,15 @@
 	return 0;
       }
 
+    madj = 0;
+    /* Ignore excess limbs in MP,MSIZE.  */
+    if (msize > prec)
+      {
+	madj = msize - prec;
+	mp += msize - prec;
+	msize = prec;
+      }      
+
     if (expflag != 0)
       exp_in_base = strtol (str + 1, (char **) 0,
 			    decimal_exponent_flag ? 10 : base);
@@ -193,85 +206,79 @@
       {
 	MPN_COPY (x->_mp_d, mp, msize);
 	x->_mp_size = negative ? -msize : msize;
-	x->_mp_exp = msize;
+	x->_mp_exp = msize + madj;
 	TMP_FREE (marker);
 	return 0;
       }
 
-#if 1
-    rsize = (((mp_size_t) (exp_in_base / __mp_bases[base].chars_per_bit_exactly))
+    ralloc = (((mp_size_t) (exp_in_base / __mp_bases[base].chars_per_bit_exactly))
 	     / BITS_PER_MP_LIMB + 3);
-#else
-    count_leading_zeros (cnt, (mp_limb_t) base);
-    rsize = exp_in_base - cnt * exp_in_base / BITS_PER_MP_LIMB + 1;
-#endif
-    rp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
-    tp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
+    rp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
+    tp = (mp_ptr) TMP_ALLOC (ralloc * BYTES_PER_MP_LIMB);
 
     rp[0] = base;
     rsize = 1;
-
     count_leading_zeros (cnt, exp_in_base);
-
     for (i = BITS_PER_MP_LIMB - cnt - 2; i >= 0; i--)
       {
 	mpn_mul_n (tp, rp, rp, rsize);
 	rsize = 2 * rsize;
 	rsize -= tp[rsize - 1] == 0;
-	xp = tp; tp = rp; rp = xp;
+	swapptr (rp, tp);
 
 	if (((exp_in_base >> i) & 1) != 0)
 	  {
+	    mp_limb_t cy;
 	    cy = mpn_mul_1 (rp, rp, rsize, (mp_limb_t) base);
 	    rp[rsize] = cy;
 	    rsize += cy != 0;
 	  }
       }
 
+    radj = 0;
     if (rsize > prec)
       {
-	xxx += rsize - prec;
+	radj += rsize - prec;
 	rp += rsize - prec;
 	rsize = prec;
       }
-#if 0
-    if (msize > prec)
-      {
-	xxx -= msize - prec;
-	mp += msize - prec;
-	msize = prec;
-      }
-#endif
+
     if (divflag)
       {
 	mp_ptr qp;
-	mp_limb_t qflag;
-	mp_size_t xtra;
-	if (msize <= rsize)
+	mp_limb_t qlimb;
+	if (msize < rsize)
 	  {
-	    /* Allocate extra limb for current divrem sematics. */
+	    /* Pad out MP,MSIZE for current divrem semantics.  */
 	    mp_ptr tmp = (mp_ptr) TMP_ALLOC ((rsize + 1) * BYTES_PER_MP_LIMB);
 	    MPN_ZERO (tmp, rsize - msize);
 	    MPN_COPY (tmp + rsize - msize, mp, msize);
 	    mp = tmp;
-	    xxx += rsize - msize;
+	    madj -= rsize - msize;
 	    msize = rsize;
 	  }
 	count_leading_zeros (cnt, rp[rsize - 1]);
 	if (cnt != 0)
 	  {
+	    mp_limb_t cy;
 	    mpn_lshift (rp, rp, rsize, cnt);
 	    cy = mpn_lshift (mp, mp, msize, cnt);
 	    if (cy)
 	      mp[msize++] = cy;
 	  }
+
 	qp = (mp_ptr) TMP_ALLOC ((prec + 1) * BYTES_PER_MP_LIMB);
-	xtra = prec - (msize - rsize);
-	qflag = mpn_divrem (qp, xtra, mp, msize, rp, rsize);
-	qp[prec] = qflag;
+	qlimb = mpn_divrem (qp, prec - (msize - rsize), mp, msize, rp, rsize);
 	tp = qp;
-	rsize = prec + qflag;
-	exp_in_limbs = rsize - xtra - xxx;
+	exp_in_limbs = qlimb + (msize - rsize) + (madj - radj);
+	rsize = prec;
+	if (qlimb != 0)
+	  {
+	    tp[prec] = qlimb;
+	    /* Skip the least significant limb not to overrun the destination
+	       variable.  */
+	    tp++;
+	  }
       }
     else
       {
@@ -282,11 +289,10 @@
 	  mpn_mul (tp, mp, msize, rp, rsize);
 	rsize += msize;
 	rsize -= tp[rsize - 1] == 0;
-	exp_in_limbs = rsize + xxx;
+	exp_in_limbs = rsize + madj + radj;
 
 	if (rsize > prec)
 	  {
-	    xxx = rsize - prec;
 	    tp += rsize - prec;
 	    rsize = prec;
 	    exp_in_limbs += 0;
--- gmp-2.0.2-vanilla/mpn/configure.in	Tue Jun  4 07:52:01 1996
+++ gmp-2.0.2/mpn/configure.in	Thu Jul  1 11:51:36 1999
@@ -12,12 +12,11 @@
 
 case "${target}" in
   sparc9*-*-* | sparc64*-*-* | ultrasparc*-*-*)
-	#path="sparc64" ;;	Don't use this until compilers are ready
-	path="sparc32/v8 sparc" ;;
+	path="sparc64" ;;
   sparc8*-*-* | microsparc*-*-*)
-	path="sparc32/v8 sparc" ;;
+	path="sparc32/v8 sparc32" ;;
   supersparc*-*-*)
-	path="sparc32/v8/supersparc sparc32/v8 sparc"
+	path="sparc32/v8/supersparc sparc32/v8 sparc32"
 	extra_functions="udiv" ;;
   sparc*-*-*) path="sparc32"
 	if [ x$floating_point = xno ]
--- gmp-2.0.2-vanilla/mpn/sparc64/addmul_1.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/addmul_1.s	Thu Jul  1 13:12:20 1999
@@ -1,7 +1,7 @@
 ! SPARC v9 __mpn_addmul_1 -- Multiply a limb vector with a single limb and
 ! add the product to a second limb vector.
 
-! Copyright (C) 1996 Free Software Foundation, Inc.
+! Copyright (C) 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -33,28 +33,26 @@
 	.type	 __mpn_addmul_1,#function
 	.proc	016
 __mpn_addmul_1:
-	!#PROLOGUE#	0
-	save	%sp,-160,%sp
-	!#PROLOGUE#	1
+	save	%sp,-192,%sp
+
 	sub	%g0,%i2,%o7
-	sllx	%o7,3,%g5
-	sub	%i1,%g5,%o3
-	sub	%i0,%g5,%o4
 	mov	0,%o0			! zero cy_limb
-
+	sllx	%o7,3,%o7
+	sethi	%hi(0x80000000),%o2
 	srl	%i3,0,%o1		! extract low 32 bits of s2_limb
+	sub	%i1,%o7,%o3
 	srlx	%i3,32,%i3		! extract high 32 bits of s2_limb
-	mov	1,%o2
-	sllx	%o2,32,%o2		! o2 = 0x100000000
+	sub	%i0,%o7,%o4
+	add	%o2,%o2,%o2		! o2 = 0x100000000
 
 	!   hi   !
              !  mid-1 !
              !  mid-2 !
 		 !   lo   !
-.Loop:
-	sllx	%o7,3,%g1
-	ldx	[%o3+%g1],%g5
+.Lloop:
+	ldx	[%o3+%o7],%g5
 	srl	%g5,0,%i0		! zero hi bits
+	ldx	[%o4+%o7],%l1
 	srlx	%g5,32,%g5
 	mulx	%o1,%i0,%i4		! lo product
 	mulx	%i3,%i0,%i1		! mid-1 product
@@ -65,25 +63,25 @@
 	addcc	%i1,%l2,%i1		! add mid products
 	mov	0,%l0			! we need the carry from that add...
 	movcs	%xcc,%o2,%l0		! ...compute it and...
+	sllx	%i1,32,%i0		!  align low bits of mid product
 	add	%i5,%l0,%i5		! ...add to bit 32 of the hi product
-	sllx	%i1,32,%i0		! align low bits of mid product
 	srl	%i4,0,%g5		! zero high 32 bits of lo product
 	add	%i0,%g5,%i0		! combine into low 64 bits of result
 	srlx	%i1,32,%i1		! extract high bits of mid product...
+	addcc	%i0,%o0,%i0		!  add cy_limb to low 64 bits of result
 	add	%i5,%i1,%i1		! ...and add them to the high result
-	addcc	%i0,%o0,%i0		! add cy_limb to low 64 bits of result
 	mov	0,%g5
 	movcs	%xcc,1,%g5
-	add	%o7,1,%o7
-	ldx	[%o4+%g1],%l1
 	addcc	%l1,%i0,%i0
-	movcs	%xcc,1,%g5
-	stx	%i0,[%o4+%g1]
-	brnz	%o7,.Loop
-	add	%i1,%g5,%o0		! compute new cy_limb
-
-	mov	%o0,%i0
-	ret
-	restore
+	stx	%i0,[%o4+%o7]
+	add	%g5,1,%l1
+	movcs	%xcc,%l1,%g5
+	addcc	%o7,8,%o7
+	bne,pt	%xcc,.Lloop
+	 add	%i1,%g5,%o0		! compute new cy_limb
+
+	jmpl	%i7+8, %g0
+	 restore %o0,%g0,%o0
+
 .LLfe1:
 	.size  __mpn_addmul_1,.LLfe1-__mpn_addmul_1
--- gmp-2.0.2-vanilla/mpn/sparc64/lshift.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/lshift.s	Thu Jul  1 13:12:28 1999
@@ -1,6 +1,6 @@
 ! SPARC v9 __mpn_lshift --
 
-! Copyright (C) 1996 Free Software Foundation, Inc.
+! Copyright (C) 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -38,22 +38,22 @@
 	sub	%g0,%o3,%o5	! negate shift count
 	add	%o0,%g1,%o0	! make %o0 point at end of res
 	add	%o2,-1,%o2
-	and	%o2,4-1,%g4	! number of limbs in first loop
+	andcc	%o2,4-1,%g4	! number of limbs in first loop
 	srlx	%g2,%o5,%g1	! compute function result
-	brz,pn	%g4,.L0		! if multiple of 4 limbs, skip first loop
-	stx	%g1,[%sp+80]
+	be,pn	%xcc,.L0	! if multiple of 4 limbs, skip first loop
+	 mov	%g1,%g5
 
 	sub	%o2,%g4,%o2	! adjust count for main loop
 
 .Loop0:	ldx	[%o1-16],%g3
 	add	%o0,-8,%o0
 	add	%o1,-8,%o1
-	add	%g4,-1,%g4
 	sllx	%g2,%o3,%o4
+	addcc	%g4,-1,%g4
 	srlx	%g3,%o5,%g1
 	mov	%g3,%g2
 	or	%o4,%g1,%o4
-	brnz,pt	%g4,.Loop0
+	bne,pt	%xcc,.Loop0
 	 stx	%o4,[%o0+0]
 
 .L0:	brz,pn	%o2,.Lend
@@ -61,8 +61,8 @@
 
 .Loop:	ldx	[%o1-16],%g3
 	add	%o0,-32,%o0
-	add	%o2,-4,%o2
 	sllx	%g2,%o3,%o4
+	addcc	%o2,-4,%o2
 	srlx	%g3,%o5,%g1
 
 	ldx	[%o1-24],%g2
@@ -85,12 +85,12 @@
 
 	add	%o1,-32,%o1
 	or	%g4,%g1,%g4
-	brnz,pt	%o2,.Loop
+	bne,pt	%xcc,.Loop
 	 stx	%g4,[%o0+0]
 
 .Lend:	sllx	%g2,%o3,%g2
 	stx	%g2,[%o0-8]
 	retl
-	ldx	[%sp+80],%o0
+	 mov	%g5,%o0
 .LLfe1:
 	.size	 __mpn_lshift,.LLfe1-__mpn_lshift
--- gmp-2.0.2-vanilla/mpn/sparc64/mul_1.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/mul_1.s	Thu Jul  1 13:19:57 1999
@@ -1,7 +1,7 @@
 ! SPARC v9 __mpn_mul_1 -- Multiply a limb vector with a single limb and
 ! store the product in a second limb vector.
 
-! Copyright (C) 1995, 1996 Free Software Foundation, Inc.
+! Copyright (C) 1995, 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -74,13 +74,12 @@
 	addcc	%i0,%o0,%i0		! add cy_limb to low 64 bits of result
 	mov	0,%g5
 	movcs	%xcc,1,%g5
-	add	%o7,1,%o7
+	addcc	%o7,1,%o7
 	stx	%i0,[%o4+%g1]
-	brnz	%o7,.Loop
-	add	%i1,%g5,%o0		! compute new cy_limb
+	bne,pt	%xcc,.Loop
+	 add	%i1,%g5,%o0		! compute new cy_limb
 
-	mov	%o0,%i0
 	ret
-	restore
+	 restore %o0,%g0,%o0
 .LLfe1:
 	.size  __mpn_mul_1,.LLfe1-__mpn_mul_1
--- gmp-2.0.2-vanilla/mpn/sparc64/rshift.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/rshift.s	Thu Jul  1 13:12:43 1999
@@ -1,6 +1,6 @@
 ! SPARC v9 __mpn_rshift --
 
-! Copyright (C) 1996 Free Software Foundation, Inc.
+! Copyright (C) 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -35,22 +35,22 @@
 	ldx	[%o1],%g2	! load first limb
 	sub	%g0,%o3,%o5	! negate shift count
 	add	%o2,-1,%o2
-	and	%o2,4-1,%g4	! number of limbs in first loop
+	andcc	%o2,4-1,%g4	! number of limbs in first loop
 	sllx	%g2,%o5,%g1	! compute function result
-	brz,pn	%g4,.L0		! if multiple of 4 limbs, skip first loop
-	stx	%g1,[%sp+80]
+	be,pn	%xcc,.L0	! if multiple of 4 limbs, skip first loop
+	 mov	%g1,%g5
 
 	sub	%o2,%g4,%o2	! adjust count for main loop
 
 .Loop0:	ldx	[%o1+8],%g3
 	add	%o0,8,%o0
 	add	%o1,8,%o1
-	add	%g4,-1,%g4
 	srlx	%g2,%o3,%o4
+	addcc	%g4,-1,%g4
 	sllx	%g3,%o5,%g1
 	mov	%g3,%g2
 	or	%o4,%g1,%o4
-	brnz,pt	%g4,.Loop0
+	bne,pt	%xcc,.Loop0
 	 stx	%o4,[%o0-8]
 
 .L0:	brz,pn	%o2,.Lend
@@ -58,8 +58,8 @@
 
 .Loop:	ldx	[%o1+8],%g3
 	add	%o0,32,%o0
-	add	%o2,-4,%o2
 	srlx	%g2,%o3,%o4
+	addcc	%o2,-4,%o2
 	sllx	%g3,%o5,%g1
 
 	ldx	[%o1+16],%g2
@@ -82,12 +82,12 @@
 
 	add	%o1,32,%o1
 	or	%g4,%g1,%g4
-	brnz	%o2,.Loop
+	bne,pt	%xcc,.Loop
 	 stx	%g4,[%o0-8]
 
 .Lend:	srlx	%g2,%o3,%g2
 	stx	%g2,[%o0-0]
 	retl
-	ldx	[%sp+80],%o0
+	 mov	%g5,%o0
 .LLfe1:
 	.size	__mpn_rshift,.LLfe1-__mpn_rshift
--- gmp-2.0.2-vanilla/mpn/sparc64/sub_n.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/sub_n.s	Thu Jul  1 13:12:50 1999
@@ -1,7 +1,7 @@
 ! SPARC v9 __mpn_sub_n -- Subtract two limb vectors of the same length > 0 and
 ! store difference in a third limb vector.
 
-! Copyright (C) 1995, 1996 Free Software Foundation, Inc.
+! Copyright (C) 1995, 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -46,7 +46,7 @@
 	ldx [%o1+%o5],%g2		! load s1 limb
 	addcc %g1,%o4,%g1		! add s2 limb and carry variable
 	movcc %xcc,0,%o4		! if carry-out, o4 was 1; clear it
-	subcc %g1,%g2,%g1		! subtract s1 limb from sum
+	subcc %g2,%g1,%g1		! subtract s1 limb from sum
 	stx %g1,[%o0+%o5]		! store result
 	add %o5,8,%o5			! increment address index
 	brnz,pt %g3,.Loop
--- gmp-2.0.2-vanilla/mpn/sparc64/submul_1.s	Wed May  8 09:10:52 1996
+++ gmp-2.0.2/mpn/sparc64/submul_1.s	Thu Jul  1 13:12:59 1999
@@ -1,7 +1,7 @@
 ! SPARC v9 __mpn_submul_1 -- Multiply a limb vector with a single limb and
 ! subtract the product from a second limb vector.
 
-! Copyright (C) 1996 Free Software Foundation, Inc.
+! Copyright (C) 1996, 1999 Free Software Foundation, Inc.
 
 ! This file is part of the GNU MP Library.
 
@@ -33,28 +33,26 @@
 	.type	 __mpn_submul_1,#function
 	.proc	016
 __mpn_submul_1:
-	!#PROLOGUE#	0
-	save	%sp,-160,%sp
-	!#PROLOGUE#	1
+	save	%sp,-192,%sp
+
 	sub	%g0,%i2,%o7
-	sllx	%o7,3,%g5
-	sub	%i1,%g5,%o3
-	sub	%i0,%g5,%o4
 	mov	0,%o0			! zero cy_limb
-
+	sllx	%o7,3,%o7
+	sethi	%hi(0x80000000),%o2
 	srl	%i3,0,%o1		! extract low 32 bits of s2_limb
+	sub	%i1,%o7,%o3
 	srlx	%i3,32,%i3		! extract high 32 bits of s2_limb
-	mov	1,%o2
-	sllx	%o2,32,%o2		! o2 = 0x100000000
+	sub	%i0,%o7,%o4
+	add	%o2,%o2,%o2		! o2 = 0x100000000
 
 	!   hi   !
              !  mid-1 !
              !  mid-2 !
 		 !   lo   !
-.Loop:
-	sllx	%o7,3,%g1
-	ldx	[%o3+%g1],%g5
+.Lloop:
+	ldx	[%o3+%o7],%g5
 	srl	%g5,0,%i0		! zero hi bits
+	ldx	[%o4+%o7],%l1
 	srlx	%g5,32,%g5
 	mulx	%o1,%i0,%i4		! lo product
 	mulx	%i3,%i0,%i1		! mid-1 product
@@ -65,25 +63,25 @@
 	addcc	%i1,%l2,%i1		! add mid products
 	mov	0,%l0			! we need the carry from that add...
 	movcs	%xcc,%o2,%l0		! ...compute it and...
+	sllx	%i1,32,%i0		!  align low bits of mid product
 	add	%i5,%l0,%i5		! ...add to bit 32 of the hi product
-	sllx	%i1,32,%i0		! align low bits of mid product
 	srl	%i4,0,%g5		! zero high 32 bits of lo product
 	add	%i0,%g5,%i0		! combine into low 64 bits of result
 	srlx	%i1,32,%i1		! extract high bits of mid product...
+	addcc	%i0,%o0,%i0		!  add cy_limb to low 64 bits of result
 	add	%i5,%i1,%i1		! ...and add them to the high result
-	addcc	%i0,%o0,%i0		! add cy_limb to low 64 bits of result
 	mov	0,%g5
 	movcs	%xcc,1,%g5
-	add	%o7,1,%o7
-	ldx	[%o4+%g1],%l1
 	subcc	%l1,%i0,%i0
-	movcs	%xcc,1,%g5
-	stx	%i0,[%o4+%g1]
-	brnz	%o7,.Loop
-	add	%i1,%g5,%o0		! compute new cy_limb
-
-	mov	%o0,%i0
-	ret
-	restore
+	stx	%i0,[%o4+%o7]
+	add	%g5,1,%l1
+	movcs	%xcc,%l1,%g5
+	addcc	%o7,8,%o7
+	bne,pt	%xcc,.Lloop
+	 add	%i1,%g5,%o0		! compute new cy_limb
+
+	jmpl	%i7+8, %g0
+	 restore %o0,%g0,%o0
+
 .LLfe1:
 	.size  __mpn_submul_1,.LLfe1-__mpn_submul_1
--- gmp-2.0.2-vanilla/mpq/add.c	Wed May  8 09:11:07 1996
+++ gmp-2.0.2/mpq/add.c	Wed Nov 11 18:31:52 1998
@@ -64,12 +64,12 @@
       MPZ_TMP_INIT (t, MAX (ABS (tmp1->_mp_size), ABS (tmp2->_mp_size)) + 1);
 
       mpz_add (t, tmp1, tmp2);
-      mpz_divexact (tmp1, &(op1->_mp_den), gcd);
+      mpz_divexact (tmp2, &(op1->_mp_den), gcd);
       mpz_gcd (gcd, t, gcd);
 
       mpz_divexact (&(rop->_mp_num), t, gcd);
 
-      mpz_divexact (tmp2, &(op2->_mp_den), gcd);
+      mpz_divexact (tmp1, &(op2->_mp_den), gcd);
       mpz_mul (&(rop->_mp_den), tmp1, tmp2);
     }
   else
--- gmp-2.0.2-vanilla/mpq/equal.c	Tue Jun  4 09:35:50 1996
+++ gmp-2.0.2/mpq/equal.c	Wed Nov 11 18:24:34 1998
@@ -38,6 +38,6 @@
   mp_size_t den2_size = op2->_mp_den._mp_size;
 
   return (num1_size == num2_size && den1_size == den2_size
-	  && mpn_cmp (op1->_mp_num._mp_d, op2->_mp_num._mp_d, num1_size) == 0
+	  && mpn_cmp (op1->_mp_num._mp_d, op2->_mp_num._mp_d, ABS (num1_size)) == 0
 	  && mpn_cmp (op1->_mp_den._mp_d, op2->_mp_den._mp_d, den1_size) == 0);
 }
--- gmp-2.0.2-vanilla/mpq/sub.c	Wed May  8 09:11:07 1996
+++ gmp-2.0.2/mpq/sub.c	Wed Nov 11 18:32:18 1998
@@ -64,12 +64,12 @@
       MPZ_TMP_INIT (t, MAX (ABS (tmp1->_mp_size), ABS (tmp2->_mp_size)) + 1);
 
       mpz_sub (t, tmp1, tmp2);
-      mpz_divexact (tmp1, &(op1->_mp_den), gcd);
+      mpz_divexact (tmp2, &(op1->_mp_den), gcd);
       mpz_gcd (gcd, t, gcd);
 
       mpz_divexact (&(rop->_mp_num), t, gcd);
 
-      mpz_divexact (tmp2, &(op2->_mp_den), gcd);
+      mpz_divexact (tmp1, &(op2->_mp_den), gcd);
       mpz_mul (&(rop->_mp_den), tmp1, tmp2);
     }
   else
--- gmp-2.0.2-vanilla/mpz/invert.c	Wed May  8 09:11:06 1996
+++ gmp-2.0.2/mpz/invert.c	Wed Nov 11 18:30:48 1998
@@ -1,6 +1,6 @@
 /* mpz_invert (inv, x, n).  Find multiplicative inverse of X in Z(N).
-   If X has an inverse, return non-zero and store inverse in INVERSE,
-   otherwise, return 0 and put garbage in X.
+   If X has an inverse, return non-zero and store inverse in INV,
+   otherwise, return 0 and put garbage in INV.
 
 Copyright (C) 1996 Free Software Foundation, Inc.
 
@@ -22,6 +22,7 @@
 MA 02111-1307, USA. */
 
 #include "gmp.h"
+#include "gmp-impl.h"
 
 int
 #if __STDC__
@@ -33,11 +34,33 @@
 #endif
 {
   mpz_t gcd;
-  int rv;
+  mp_size_t xsize, nsize, size;
 
-  mpz_init (gcd);
+  xsize = SIZ (x);
+  nsize = SIZ (n);
+  xsize = ABS (xsize);
+  nsize = ABS (nsize);
+  size = MAX (xsize, nsize) + 1;
+
+  /* No inverse exists if the leftside operand is 0.  Likewise, no
+     inverse exists if the mod operand is 1.  */
+  if (xsize == 0 || (nsize == 1 && (PTR (n))[0] == 1))
+    return 0;
+
+  MPZ_TMP_INIT (gcd, size);
   mpz_gcdext (gcd, inverse, (mpz_ptr) 0, x, n);
-  rv = gcd->_mp_size == 1 && (gcd->_mp_d)[0] == 1;
-  mpz_clear (gcd);
-  return rv;
+
+  /* If no inverse existed, return with an indication of that.  */
+  if (gcd->_mp_size != 1 || (gcd->_mp_d)[0] != 1)
+    return 0;
+
+  /* Make sure we return a positive inverse.  */
+  if (SIZ (inverse) < 0)
+    {
+      if (SIZ (n) < 0)
+	mpz_sub (inverse, inverse, n);
+      else
+	mpz_add (inverse, inverse, n);
+    }
+  return 1;
 }
--- gmp-2.0.2-vanilla/mpz/tests/t-powm_ui.c	Wed May  8 09:11:07 1996
+++ gmp-2.0.2/mpz/tests/t-powm_ui.c	Wed Nov 11 18:25:40 1998
@@ -37,7 +37,8 @@
   mpz_t base, mod;
   mpz_t r1, r2, base2;
   mp_size_t base_size, mod_size;
-  mp_limb_t exp, exp2;
+  mp_limb_t exp;
+  unsigned long int exp2;
   int i;
   int reps = 10000;
 
